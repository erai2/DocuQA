"""High level helper that turns rules + ì§ˆë¬¸ into an LLM answer."""

from __future__ import annotations

import os
from functools import lru_cache
from typing import Any, Dict, Optional

from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate

try:  # pragma: no cover - optional dependency alias
    from langchain_openai import ChatOpenAI
except ImportError:  # pragma: no cover
    from langchain.chat_models import ChatOpenAI  # type: ignore

from .analyzer import summarise_context
from .context_builder import build_context

__all__ = ["ask_suam"]

MODEL_NAME = "gpt-4o-mini"
load_dotenv()


@lru_cache(maxsize=1)
def _get_llm() -> ChatOpenAI:
    """Initialise the OpenAI chat model lazily."""

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError(
            "OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    # The OpenAI client reads the environment variable automatically.
    return ChatOpenAI(model=MODEL_NAME, temperature=0)


def _build_prompt(context: Dict[str, Any]) -> ChatPromptTemplate:
    template = """
    ë‹¹ì‹ ì€ ìˆ˜ì•”ëª…ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ rules.jsonì˜ í•´ì„ ìˆœì„œì™€ ê·œì¹™ì„ ë”°ë¥´ì‹­ì‹œì˜¤.

    ğŸ”¹ í•´ì„ ìˆœì„œ:
    {í•´ì„ìˆœì„œ}

    ğŸ”¹ ì‚¬ê±´ ì¡°ê±´:
    {ì‚¬ê±´ì¡°ê±´}

    ğŸ”¹ ì˜ˆì™¸ ê·œì¹™:
    {ì˜ˆì™¸ê·œì¹™}

    ğŸ”¹ ì‹­ì„± ë³€ë™ ê·œì¹™:
    {ì‹­ì„±ë³€ë™}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {ì§ˆë¬¸}
    ì‚¬ì£¼ ë°ì´í„°: {ì‚¬ì£¼}

    ğŸ‘‰ ìœ„ êµ¬ì¡°ë¥¼ ë”°ë¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ê³ , í˜„ì‹¤ ì‚¬ê±´ ì¤‘ì‹¬ì˜ ê²°ë¡ ì„ ì œì‹œí•˜ì„¸ìš”.
    """

    return ChatPromptTemplate.from_template(template)


def ask_suam(user_question: str, saju_data: Optional[Dict[str, Any]] = None) -> str:
    """Return an expert style answer or a deterministic fallback summary."""

    context = build_context(user_question, saju_data)
    prompt = _build_prompt(context)

    def _format_prompt_value(value: Any) -> Any:
        if isinstance(value, list):
            return "\n".join(f"- {item}" for item in value)
        if isinstance(value, dict):
            return "\n".join(f"- {key}: {val}" for key, val in value.items())
        return value

    prompt_arguments = {
        key: _format_prompt_value(context.get(key))
        for key in ("í•´ì„ìˆœì„œ", "ì‚¬ê±´ì¡°ê±´", "ì˜ˆì™¸ê·œì¹™", "ì‹­ì„±ë³€ë™", "ì§ˆë¬¸", "ì‚¬ì£¼")
    }

    try:
        llm = _get_llm()
        messages = prompt.format_messages(**prompt_arguments)
        result = llm.invoke(messages)
        content = getattr(result, "content", str(result))
        if warning := context.get("rules_warning"):
            content += "\n\nâš ï¸ ê·œì¹™ íŒŒì¼ ê²½ê³ : " + warning
        return content
    except RuntimeError as exc:
        # Missing API key or rules file error.
        fallback = summarise_context(context)
        return f"âš ï¸ {exc}\n\n{fallback}"
    except Exception:  # pragma: no cover - defensive guard
        fallback = summarise_context(context)
        return (
            "âš ï¸ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì½˜ì†” ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n\n" + fallback
        )
