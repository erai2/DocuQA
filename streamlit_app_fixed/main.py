import streamlit as st
import os
import pandas as pd
from io import StringIO

# =============================
# core ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
# =============================
from core.database import (
    ensure_db,
    insert_csv_to_db,
    load_csv_from_db,
    load_csv_files,
    list_tables,
)
from core.hybrid_search import hybrid_search
from core.ai_engine import (
    generate_ai_response,
    ask_csv_ai,
    summarize_long_csv,
    summarize_by_keywords,
)
from core.ai_utils import clean_text_with_ai
from core.rag import build_databases
from profiles_page import profiles_page

# =============================
# ì´ˆê¸° ì„¸íŒ…
# =============================
st.set_page_config(page_title="Suri Q&AI", layout="wide")
st.title("ğŸ“Š Suri Q&AI (ìµœì‹  OpenAI API ë²„ì „)")

for path in ["data", "data/raw_docs", "data/vector_db"]:
    os.makedirs(path, exist_ok=True)
ensure_db()

# =============================
# í˜ì´ì§€ ë¼ìš°íŒ…
# =============================
PAGES = {
    "ë¬¸ì„œ ê´€ë¦¬": "main_page",
    "ì¸ë¬¼ í”„ë¡œí•„": profiles_page,
}
page_choice = st.sidebar.radio("ğŸ“Œ í˜ì´ì§€ ì„ íƒ", list(PAGES.keys()))

# =============================
# 1. ë¬¸ì„œ ê´€ë¦¬ í˜ì´ì§€
# =============================
if page_choice == "ë¬¸ì„œ ê´€ë¦¬":

    # -------------------------
    # 1-1. ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±
    # -------------------------
    st.header("ğŸ“‘ ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±")

    parser_mode = st.radio(
        "íŒŒì„œ ëª¨ë“œ ì„ íƒ",
        ["1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ (ë¹ ë¦„)", "2ë‹¨ê³„: AI ë³´ì¡° (ì •ë°€)", "3ë‹¨ê³„: Hybrid (íš¨ìœ¨ì )"],
        horizontal=True
    )

    uploaded_files = st.file_uploader(
        "txt/md íŒŒì¼ ì—…ë¡œë“œ", type=["txt", "md"], accept_multiple_files=True
    )

    if uploaded_files:
        for uploaded_file in uploaded_files:
            file_content = uploaded_file.read().decode("utf-8")
            st.subheader(f"ğŸ“„ {uploaded_file.name}")
            st.text_area("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", file_content[:1000], height=200)

            if st.button(f"ì´ ë¬¸ì„œ íŒŒì‹±í•˜ê¸°: {uploaded_file.name}"):
                save_path = os.path.join("data/raw_docs", uploaded_file.name)
                with open(save_path, "w", encoding="utf-8") as f:
                    f.write(file_content)

                # íŒŒì„œ ëª¨ë“œ ì„ íƒ
                rows = []
                if "ê·œì¹™ ê¸°ë°˜" in parser_mode:
                    from core.parsing import parse_document
                    cases, rules, concepts = parse_document(file_content)
                elif "AI ë³´ì¡°" in parser_mode:
                    from core.parse_document_ml import parse_document_ml
                    cases, rules, concepts = parse_document_ml(file_content)
                else:
                    from core.parse_document_hybrid import parse_document_hybrid
                    cases, rules, concepts = parse_document_hybrid(file_content)

                # ê²°ê³¼ â†’ DataFrame
                for c in cases:
                    rows.append({"type": "case", "id": c["id"], "content": c.get("detail", "")})
                for r in rules:
                    rows.append({"type": "rule", "id": r["id"], "content": r.get("desc", "")})
                for c in concepts:
                    rows.append({"type": "concept", "id": c["id"], "content": c.get("desc", "")})
                parsed_df = pd.DataFrame(rows)

                if parsed_df is not None and not parsed_df.empty:
                    st.success("âœ… íŒŒì‹± ì™„ë£Œ, AI êµì • ì ìš© ì¤‘...")

                    raw_text = parsed_df.to_csv(index=False, encoding="utf-8-sig")
                    cleaned_text = clean_text_with_ai(raw_text)

                    try:
                        cleaned_df = pd.read_csv(StringIO(cleaned_text))
                    except Exception as e:
                        st.error(f"AI êµì • í›„ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")
                        cleaned_df = parsed_df

                    st.success("âœ… AI êµì • ì™„ë£Œ! ì•„ë˜ì—ì„œ ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•˜ì„¸ìš”.")
                    edited_df = st.data_editor(cleaned_df, num_rows="dynamic", use_container_width=True)

                    if st.button(f"{uploaded_file.name} ì €ì¥", key=f"save_{uploaded_file.name}"):
                        parsed_csv = "data/parsed_docs.csv"
                        if os.path.exists(parsed_csv):
                            old_df = pd.read_csv(parsed_csv)
                            combined = pd.concat([old_df, edited_df], ignore_index=True).drop_duplicates()
                        else:
                            combined = edited_df

                        combined.to_csv(parsed_csv, index=False, encoding="utf-8-sig")
                        st.success(f"ğŸ“‚ parsed_docs.csv ì €ì¥ ì™„ë£Œ (ì´ {len(combined)}í–‰) âœ…")

                        total_rows = insert_csv_to_db(combined, table_name="parsed_docs")
                        st.success(f"ğŸ“¦ DB ì €ì¥ ì™„ë£Œ: {total_rows}í–‰ (ì¤‘ë³µ ì œê±° í›„)")
                else:
                    st.warning("âš ï¸ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # -------------------------
    # 1-2. DB ë°ì´í„° í™•ì¸
    # -------------------------
    st.header("ğŸ“¦ DB ë°ì´í„° í™•ì¸")
    if st.button("DBì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"):
        db_df = load_csv_from_db("parsed_docs")
        if not db_df.empty:
            st.subheader("ğŸ“¦ DB ë¶ˆëŸ¬ì˜¤ê¸° ê²°ê³¼")
            st.dataframe(db_df, use_container_width=True)
        else:
            st.warning("âš ï¸ DBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # -------------------------
    # 1-3. CSV ìš”ì•½
    # -------------------------
    st.header("ğŸ“ CSV ìš”ì•½")
    if st.button("CSV ì „ì²´ ìš”ì•½"):
        csv_dfs = load_csv_files("data")
        if not csv_dfs:
            st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            try:
                combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
                csv_text = combined_df.to_csv(index=False, encoding="utf-8-sig")
                summary, parts = summarize_long_csv(csv_text)
                st.text_area("CSV ì „ì²´ ìš”ì•½ ê²°ê³¼", summary, height=300)
                with st.expander("ë¶€ë¶„ ìš”ì•½ ë³´ê¸°"):
                    for part in parts:
                        st.markdown(part)
            except ValueError as exc:
                st.error(f"CSV ê²°í•© ì˜¤ë¥˜: {exc}")

    # -------------------------
    # 1-4. í‚¤ì›Œë“œë³„ ì •ë¦¬
    # -------------------------
    st.header("ğŸ”‘ í‚¤ì›Œë“œë³„ ë¬¸ì„œ ì •ë¦¬")
    keywords_input = st.text_input("í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆ(,)ë¡œ ì…ë ¥ (ì˜ˆ: ì¬ë¬¼, í˜¼ì¸, ì§ì¥, ê±´ê°•)")
    if st.button("í‚¤ì›Œë“œë³„ ì •ë¦¬ ì‹¤í–‰"):
        csv_dfs = load_csv_files("data")
        if not csv_dfs:
            st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
            csv_text = combined_df.to_csv(index=False, encoding="utf-8-sig")
            keywords = [kw.strip() for kw in keywords_input.split(",") if kw.strip()]
            if not keywords:
                st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            else:
                summary_by_kw = summarize_by_keywords(csv_text, keywords)
                st.text_area("í‚¤ì›Œë“œë³„ ì •ë¦¬ ê²°ê³¼", summary_by_kw, height=400)

    # -------------------------
    # 1-5. AI ìƒë‹´
    # -------------------------
    st.header("ğŸ’¬ AI ìƒë‹´")
    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="user_query")
    search_mode = st.radio("ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ", ["ë²¡í„° ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"], horizontal=True)

    if st.button("AI ì‘ë‹µ ìƒì„±"):
        if query.strip():
            if search_mode == "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰":
                docs = hybrid_search(query, db_dir="data/vector_db", k=5)
                context = "\n\n".join([d.page_content for d in docs])
                answer = generate_ai_response(f"{query}\n\nì°¸ê³ ìë£Œ:\n{context}")
            else:
                answer = generate_ai_response(query)
            st.markdown(answer)
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

    # -------------------------
    # 1-6. DB ë¹Œë“œ
    # -------------------------
    st.header("ğŸ› ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ")
    if st.button("ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ ì‹¤í–‰"):
        with st.spinner("ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê³  DB/VectorDBë¥¼ ë¹Œë“œ ì¤‘..."):
            vs = build_databases(data_dir="data/raw_docs", db_dir="data/vector_db")
        if vs:
            st.success("âœ… DB ë° VectorDB ë¹Œë“œ ì™„ë£Œ")
        else:
            st.warning("âš ï¸ ë¹Œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # -------------------------
    # 1-7. DB ê´€ë¦¬
    # -------------------------
    st.header("ğŸ—‚ï¸ DB ê´€ë¦¬")
    if st.button("í…Œì´ë¸” ëª©ë¡ ë³´ê¸°"):
        tables = list_tables()
        if tables:
            st.write("ğŸ“‹ í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:")
            st.write(tables)
        else:
            st.info("DBì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")

    tables = list_tables()
    if tables:
        selected_table = st.selectbox("ì¡°íšŒí•  í…Œì´ë¸” ì„ íƒ", tables, key="view_table")
        if st.button("í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°"):
            df = load_csv_from_db(selected_table)
            if not df.empty:
                st.dataframe(df, use_container_width=True)
            else:
                st.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        del_table = st.selectbox("ì‚­ì œí•  í…Œì´ë¸” ì„ íƒ", tables, key="delete_table")
        if st.button("í…Œì´ë¸” ì‚­ì œ"):
            import sqlite3
            conn = sqlite3.connect("suri_m.db")
            cur = conn.cursor()
            cur.execute(f"DROP TABLE IF EXISTS {del_table}")
            conn.commit()
            conn.close()
            st.success(f"ğŸ—‘ï¸ {del_table} í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")

# =============================
# 2. ì¸ë¬¼ í”„ë¡œí•„ í˜ì´ì§€
# =============================
elif page_choice == "ì¸ë¬¼ í”„ë¡œí•„":
    profiles_page()
