"""Streamlit entrypoint for the Suri Q&AI application."""

from __future__ import annotations

import os
from io import StringIO
from typing import Dict, Iterable, List

import pandas as pd
import streamlit as st

from core.ai_engine import generate_ai_response, summarize_by_keywords, summarize_long_csv
from core.ai_utils import clean_text_with_ai
from core.database import (
    ensure_db,
    insert_csv_to_db,
    load_csv_files,
    load_csv_from_db,
    list_tables,
)
from core.hybrid_search import hybrid_search
from core.rag import build_databases
from profiles_page import profiles_page


DATA_DIR = "data"
RAW_DOCS_DIR = os.path.join(DATA_DIR, "raw_docs")
VECTOR_DB_DIR = os.path.join(DATA_DIR, "vector_db")
PARSED_CSV_PATH = os.path.join(DATA_DIR, "parsed_docs.csv")

RULE_CATEGORIES = [
    "ê¶ìœ„ (Gung-wi)",
    "ì‹­ì‹  í—ˆíˆ¬ (Heo-tu)",
    "ëŒ€ìƒ (Dae-sang)",
    "ë¬˜ê³  (Mooko)",
    "ìƒí˜¸ì‘ìš© ê·œì¹™",
]

PRIORITY_LEVELS = [
    "1. êµ¬ì¡° ê²°ì •",
    "2. íŠ¹ìˆ˜ êµ¬ì¡°",
    "3. í—ˆíˆ¬/ì…ë¬˜",
    "4. ì¼ë°˜ë¡ ",
]

CASE_FOCUS_TOPICS = [
    "ì§ì—…/ì‚¬íšŒìš´",
    "ê°€ì¡±/ìœ¡ì¹œ",
    "ì¬ë¬¼ìš´",
    "ê±´ê°•/í•™ì—…ìš´",
]


def configure_app() -> None:
    """Set default layout information and bootstrap directories/database."""

    st.set_page_config(page_title="Suri Q&AI", layout="wide")
    st.title("ğŸ“Š Suri Q&AI (ìµœì‹  OpenAI API ë²„ì „)")

    ensure_directories([DATA_DIR, RAW_DOCS_DIR, VECTOR_DB_DIR])
    ensure_db()


def ensure_directories(paths: Iterable[str]) -> None:
    """Create directories required by the application if they do not exist."""

    for path in paths:
        os.makedirs(path, exist_ok=True)


def render_document_management_page() -> None:
    """Render the document management workflow."""

    render_upload_section()
    render_db_preview_section()
    render_rule_case_dashboard()
    render_csv_summary_section()
    render_keyword_summary_section()
    render_chatbot_workflow_section()
    render_ai_consultation_section()
    render_database_build_section()
    render_database_management_section()


def render_upload_section() -> None:
    st.header("ğŸ“‘ ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±")

    parser_mode = st.radio(
        "íŒŒì„œ ëª¨ë“œ ì„ íƒ",
        ["1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ (ë¹ ë¦„)", "2ë‹¨ê³„: AI ë³´ì¡° (ì •ë°€)", "3ë‹¨ê³„: Hybrid (íš¨ìœ¨ì )"],
        horizontal=True,
    )

    uploaded_files = st.file_uploader(
        "txt/md íŒŒì¼ ì—…ë¡œë“œ", type=["txt", "md"], accept_multiple_files=True
    )

    if not uploaded_files:
        return

    for uploaded_file in uploaded_files:
        file_content = uploaded_file.read().decode("utf-8")
        st.subheader(f"ğŸ“„ {uploaded_file.name}")
        st.text_area("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", file_content[:1000], height=200)

        if not st.button(f"ì´ ë¬¸ì„œ íŒŒì‹±í•˜ê¸°: {uploaded_file.name}"):
            continue

        save_path = os.path.join(RAW_DOCS_DIR, uploaded_file.name)
        with open(save_path, "w", encoding="utf-8") as file:
            file.write(file_content)

        parsed_df = parse_document_by_mode(file_content, parser_mode)
        if parsed_df.empty:
            st.warning("âš ï¸ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        st.success("âœ… íŒŒì‹± ì™„ë£Œ, AI êµì • ì ìš© ì¤‘...")

        raw_text = parsed_df.to_csv(index=False, encoding="utf-8-sig")
        cleaned_df = clean_dataframe_with_ai(raw_text, parsed_df)

        st.success("âœ… AI êµì • ì™„ë£Œ! ì•„ë˜ì—ì„œ ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•˜ì„¸ìš”.")
        edited_df = st.data_editor(
            cleaned_df,
            num_rows="dynamic",
            use_container_width=True,
            column_config=build_editor_column_config(),
            column_order=[
                "type",
                "id",
                "category",
                "priority_tier",
                "focus_topic",
                "keywords",
                "content",
                "principle_summary",
                "analysis",
                "chart",
                "day_master",
                "core_principles",
                "source_document",
            ],
            disabled=["type", "id"],
        )

        if st.button(f"{uploaded_file.name} ì €ì¥", key=f"save_{uploaded_file.name}"):
            combined = merge_with_existing_csv(edited_df)
            combined.to_csv(PARSED_CSV_PATH, index=False, encoding="utf-8-sig")
            st.success(f"ğŸ“‚ parsed_docs.csv ì €ì¥ ì™„ë£Œ (ì´ {len(combined)}í–‰) âœ…")

            total_rows = insert_csv_to_db(combined, table_name="parsed_docs")
            st.success(f"ğŸ“¦ DB ì €ì¥ ì™„ë£Œ: {total_rows}í–‰ (ì¤‘ë³µ ì œê±° í›„)")


def parse_document_by_mode(file_content: str, parser_mode: str) -> pd.DataFrame:
    """Parse documents using the selected parsing strategy and enrich with taxonomy fields."""

    rows: List[Dict[str, str]] = []

    if "ê·œì¹™ ê¸°ë°˜" in parser_mode:
        from core.parsing import parse_document

        cases, rules, concepts = parse_document(file_content)
    elif "AI ë³´ì¡°" in parser_mode:
        from core.parse_document_ml import parse_document_ml

        cases, rules, concepts = parse_document_ml(file_content)
    else:
        from core.parse_document_hybrid import parse_document_hybrid

        cases, rules, concepts = parse_document_hybrid(file_content)

    for case in cases:
        rows.append({"type": "case", "id": case["id"], "content": case.get("detail", "")})
    for rule in rules:
        rows.append({"type": "rule", "id": rule["id"], "content": rule.get("desc", "")})
    for concept in concepts:
        rows.append({"type": "concept", "id": concept["id"], "content": concept.get("desc", "")})

    return build_structured_dataframe(rows)


def build_structured_dataframe(rows: List[Dict[str, str]]) -> pd.DataFrame:
    """Return a DataFrame seeded with ìˆ˜ì•” ëª…ë¦¬ taxonomy columns for downstream editing."""

    df = pd.DataFrame(rows)
    if df.empty:
        return df

    df = df.fillna("")
    df["category"] = df["type"].map(
        {
            "case": "ì‚¬ë¡€",
            "rule": "ë¯¸ë¶„ë¥˜",
            "concept": "ë¯¸ë¶„ë¥˜",
        }
    ).fillna("ë¯¸ë¶„ë¥˜")
    df["priority_tier"] = df["type"].map(
        {
            "case": "",
            "rule": PRIORITY_LEVELS[0],
            "concept": PRIORITY_LEVELS[0],
        }
    )
    df["focus_topic"] = df["type"].map({"case": CASE_FOCUS_TOPICS[0]}).fillna("")
    df["keywords"] = ""
    df["source_document"] = ""
    df["principle_summary"] = ""
    df["analysis"] = ""
    df["chart"] = ""
    df["day_master"] = ""
    df["core_principles"] = ""

    # Ensure deterministic column order for editors and CSV exports
    ordered_columns = [
        "type",
        "id",
        "category",
        "priority_tier",
        "focus_topic",
        "keywords",
        "content",
        "principle_summary",
        "analysis",
        "chart",
        "day_master",
        "core_principles",
        "source_document",
    ]
    existing_columns = [col for col in ordered_columns if col in df.columns]
    remaining_columns = [col for col in df.columns if col not in existing_columns]
    return df[existing_columns + remaining_columns]


def build_editor_column_config() -> Dict[str, st.column_config.BaseColumn]:
    """Build Streamlit column configuration for the structured editor."""

    return {
        "type": st.column_config.TextColumn("ë¶„ë¥˜", help="ì›ë³¸ ì¶”ì¶œ íƒ€ì… (ì‚¬ë¡€/ê·œì¹™/ê°œë…)"),
        "id": st.column_config.TextColumn("ì›ë¬¸ ID"),
        "category": st.column_config.SelectboxColumn(
            "í•µì‹¬ ì¹´í…Œê³ ë¦¬",
            options=["ë¯¸ë¶„ë¥˜"] + RULE_CATEGORIES + ["ì‚¬ë¡€"],
            help="ìˆ˜ì•” ëª…ë¦¬ í•µì‹¬ ì›ë¦¬ ë¶„ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.",
        ),
        "priority_tier": st.column_config.SelectboxColumn(
            "ìš°ì„ ìˆœìœ„",
            options=[""] + PRIORITY_LEVELS,
            help="ìˆ˜ì•” ëª…ë¦¬ êµ¬ì¡°ë¡  ìš°ì„ ìˆœìœ„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.",
        ),
        "focus_topic": st.column_config.SelectboxColumn(
            "ì£¼ìš” í•´ì„ ì£¼ì œ",
            options=[""] + CASE_FOCUS_TOPICS,
            help="ì‚¬ë¡€ì˜ í•´ì„ ì£¼ì œë¥¼ íƒœê·¸í•˜ì„¸ìš”.",
        ),
        "keywords": st.column_config.TextColumn(
            "í•µì‹¬ í‚¤ì›Œë“œ",
            help="ê²€ìƒ‰ìš© í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆë¡œ ì…ë ¥í•˜ì„¸ìš”.",
        ),
        "content": st.column_config.TextColumn(
            "ì›ë¬¸/ì„¤ëª…",
            help="ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”.",
        ),
        "principle_summary": st.column_config.TextColumn(
            "ì›ë¦¬ ìš”ì•½",
            help="ê°„ëµí•œ ìš”ì•½ ë˜ëŠ” ì •ë¦¬ ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.",
        ),
        "analysis": st.column_config.TextColumn(
            "í•´ì„ ë…¸íŠ¸",
            help="êµ¬ì¡° íŒë‹¨ ë˜ëŠ” ì¶”ê°€ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ì„¸ìš”.",
        ),
        "chart": st.column_config.TextColumn(
            "ëª…ì‹ (ì‚¬ë¡€)",
            help="ì‚¬ë¡€ì¼ ê²½ìš° ì‚¬ì£¼ ëª…ì‹ì„ ì…ë ¥í•˜ì„¸ìš”.",
        ),
        "day_master": st.column_config.TextColumn(
            "ì¼ê°„",
            help="ì‚¬ë¡€ì˜ ì¼ê°„ì„ ê¸°ì…í•˜ì„¸ìš”.",
        ),
        "core_principles": st.column_config.TextColumn(
            "ì ìš© ì›ì¹™",
            help="ì‚¬ë¡€ì—ì„œ í™œìš©ëœ í•µì‹¬ ì›ì¹™ì„ ë‚˜ì—´í•˜ì„¸ìš”.",
        ),
        "source_document": st.column_config.TextColumn(
            "ì¶œì²˜ ë¬¸ì„œ",
            help="ìë£Œì˜ ì¶œì²˜ íŒŒì¼ëª…ì„ ê¸°ë¡í•˜ì„¸ìš”.",
        ),
    }


def clean_dataframe_with_ai(raw_text: str, fallback_df: pd.DataFrame) -> pd.DataFrame:
    """Apply AI-powered cleaning to the parsed dataframe, falling back to original data."""

    cleaned_text = clean_text_with_ai(raw_text)
    try:
        cleaned_df = pd.read_csv(StringIO(cleaned_text))
        return align_to_reference_columns(cleaned_df, fallback_df.columns.tolist())
    except Exception as exc:  # pragma: no cover - defensive logging for UI
        st.error(f"AI êµì • í›„ CSV ë³€í™˜ ì‹¤íŒ¨: {exc}")
        return fallback_df


def merge_with_existing_csv(edited_df: pd.DataFrame) -> pd.DataFrame:
    """Merge edited dataframe with existing parsed CSV data, removing duplicates."""

    if os.path.exists(PARSED_CSV_PATH):
        old_df = pd.read_csv(PARSED_CSV_PATH).fillna("")
        target_columns = build_union_columns(edited_df, old_df)
        old_df_aligned = align_to_reference_columns(old_df, target_columns)
        new_df_aligned = align_to_reference_columns(edited_df, target_columns)
        combined = pd.concat([old_df_aligned, new_df_aligned], ignore_index=True)
        combined = deduplicate_structured_rows(combined)
        return combined

    return edited_df


def build_union_columns(*frames: pd.DataFrame) -> List[str]:
    """Return a deterministic union of columns, preserving order of the first dataframe."""

    if not frames:
        return []

    ordered = list(frames[0].columns)
    for frame in frames[1:]:
        for column in frame.columns:
            if column not in ordered:
                ordered.append(column)
    return ordered


def align_to_reference_columns(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
    """Ensure a dataframe contains the reference columns, filling missing values."""

    aligned = df.copy()
    for column in columns:
        if column not in aligned.columns:
            aligned[column] = ""
    aligned = aligned.fillna("")
    # Preserve reference order while keeping any additional columns at the end
    extra_columns = [col for col in aligned.columns if col not in columns]
    return aligned[columns + extra_columns]


def deduplicate_structured_rows(df: pd.DataFrame) -> pd.DataFrame:
    """Drop duplicated structured rows while respecting key ìˆ˜ì•” ëª…ë¦¬ columns."""

    candidate_columns = [
        "type",
        "id",
        "content",
        "category",
        "chart",
        "day_master",
        "analysis",
    ]
    subset = [col for col in candidate_columns if col in df.columns]
    if not subset:
        return df.drop_duplicates()
    return df.drop_duplicates(subset=subset)


def render_db_preview_section() -> None:
    st.header("ğŸ“¦ DB ë°ì´í„° í™•ì¸")

    if not st.button("DBì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"):
        return

    db_df = load_csv_from_db("parsed_docs")
    if db_df.empty:
        st.warning("âš ï¸ DBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.subheader("ğŸ“¦ DB ë¶ˆëŸ¬ì˜¤ê¸° ê²°ê³¼")
    st.dataframe(db_df, use_container_width=True)


def render_csv_summary_section() -> None:
    st.header("ğŸ“ CSV ìš”ì•½")

    if not st.button("CSV ì „ì²´ ìš”ì•½"):
        return

    csv_text = build_combined_csv_text()
    if not csv_text:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        summary, parts = summarize_long_csv(csv_text)
    except ValueError as exc:
        st.error(f"CSV ê²°í•© ì˜¤ë¥˜: {exc}")
        return

    st.text_area("CSV ì „ì²´ ìš”ì•½ ê²°ê³¼", summary, height=300)
    with st.expander("ë¶€ë¶„ ìš”ì•½ ë³´ê¸°"):
        for part in parts:
            st.markdown(part)


def build_combined_csv_text() -> str:
    """Combine CSV files from the data directory into a single CSV text blob."""

    csv_dfs = load_csv_files(DATA_DIR)
    if not csv_dfs:
        return ""

    combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
    return combined_df.to_csv(index=False, encoding="utf-8-sig")


def render_keyword_summary_section() -> None:
    st.header("ğŸ”‘ í‚¤ì›Œë“œë³„ ë¬¸ì„œ ì •ë¦¬")
    keywords_input = st.text_input("í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆ(,)ë¡œ ì…ë ¥ (ì˜ˆ: ì¬ë¬¼, í˜¼ì¸, ì§ì¥, ê±´ê°•)")

    if not st.button("í‚¤ì›Œë“œë³„ ì •ë¦¬ ì‹¤í–‰"):
        return

    csv_text = build_combined_csv_text()
    if not csv_text:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    keywords = [keyword.strip() for keyword in keywords_input.split(",") if keyword.strip()]
    if not keywords:
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return

    summary_by_kw = summarize_by_keywords(csv_text, keywords)
    st.text_area("í‚¤ì›Œë“œë³„ ì •ë¦¬ ê²°ê³¼", summary_by_kw, height=400)


def render_rule_case_dashboard() -> None:
    st.header("ğŸ“š ìˆ˜ì•” ëª…ë¦¬ Rule/Case ëŒ€ì‹œë³´ë“œ")

    df = load_csv_from_db("parsed_docs")
    if df.empty:
        st.info("DBì— êµ¬ì¡°í™”ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ íŒŒì‹±í•´ ì €ì¥í•˜ì„¸ìš”.")
        return

    df = df.fillna("")
    rule_df = df[df["type"] != "case"]
    case_df = df[df["type"] == "case"]

    cols = st.columns(2)
    with cols[0]:
        st.subheader("í•µì‹¬ ì›ë¦¬ ì¹´í…Œê³ ë¦¬ ë¶„í¬")
        if rule_df.empty:
            st.write("ì›ë¦¬ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
        else:
            category_counts = (
                rule_df.groupby("category")["content"].count().reset_index(name="entries")
            )
            priority_counts = (
                rule_df.groupby("priority_tier")["content"].count().reset_index(name="entries")
            )
            st.dataframe(category_counts, use_container_width=True)
            with st.expander("ìš°ì„ ìˆœìœ„ë³„ ì›ë¦¬ í˜„í™©"):
                st.dataframe(priority_counts, use_container_width=True)

    with cols[1]:
        st.subheader("ì‚¬ë¡€ íƒœê·¸ í˜„í™©")
        if case_df.empty:
            st.write("ì‚¬ë¡€ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
        else:
            focus_counts = (
                case_df.groupby("focus_topic")["content"].count().reset_index(name="entries")
            )
            st.dataframe(focus_counts, use_container_width=True)

    if not case_df.empty:
        st.markdown("---")
        focus_topic = st.selectbox(
            "ìì„¸íˆ ë³´ê³  ì‹¶ì€ ì£¼ì œ ì„ íƒ",
            ["ì „ì²´"] + CASE_FOCUS_TOPICS,
            key="case_focus_filter",
        )
        filtered_cases = (
            case_df if focus_topic == "ì „ì²´" else case_df[case_df["focus_topic"] == focus_topic]
        )
        if filtered_cases.empty:
            st.warning("í•´ë‹¹ ì£¼ì œë¡œ íƒœê·¸ëœ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            view_columns = [
                col
                for col in [
                    "chart",
                    "day_master",
                    "core_principles",
                    "analysis",
                    "source_document",
                ]
                if col in filtered_cases.columns
            ]
            st.dataframe(filtered_cases[view_columns + ["content"]], use_container_width=True)


def render_chatbot_workflow_section() -> None:
    st.header("ğŸ”® ìˆ˜ì•” ëª…ë¦¬ ì±—ë´‡ í•´ì„ ì—”ì§„")

    col1, col2 = st.columns(2)
    with col1:
        birth_year = st.text_input("ì¶œìƒ ë…„ (YYYY)", key="suan_birth_year")
        birth_month = st.text_input("ì¶œìƒ ì›” (MM)", key="suan_birth_month")
        birth_day = st.text_input("ì¶œìƒ ì¼ (DD)", key="suan_birth_day")
        birth_hour = st.text_input("ì¶œìƒ ì‹œ (HH ë˜ëŠ” ì§€ì§€)", key="suan_birth_hour")
    with col2:
        gender = st.selectbox(
            "ì„±ë³„", ["ì„ íƒ ì•ˆ í•¨", "ë‚¨ì„±", "ì—¬ì„±"], key="suan_gender_select"
        )
        focus_topic = st.selectbox(
            "ê°€ì¥ ê¶ê¸ˆí•œ ë¶„ì•¼", CASE_FOCUS_TOPICS, key="suan_focus_topic_select"
        )
        additional_question = st.text_area(
            "ì¶”ê°€ ì§ˆë¬¸ ë˜ëŠ” ìƒí™©", height=120, key="suan_additional_question"
        )

    if st.button("ìˆ˜ì•” ëª…ë¦¬ í•´ì„ ìƒì„±"):
        if not (birth_year and birth_month and birth_day and birth_hour):
            st.warning("ìƒë…„ì›”ì¼ì‹œ ì •ë³´ë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")
            return

        with st.spinner("ìˆ˜ì•” ëª…ë¦¬ êµ¬ì¡° ë¶„ì„ ì¤‘..."):
            context_payload = load_structured_context()
            prompt = build_interpretation_prompt(
                birth_year,
                birth_month,
                birth_day,
                birth_hour,
                gender,
                focus_topic,
                additional_question,
                context_payload,
            )
            response = generate_ai_response(prompt)

        st.markdown("### ğŸ§­ ìˆ˜ì•” ëª…ë¦¬ 3ë‹¨ê³„ í•´ì„")
        st.markdown(response)
        with st.expander("ì‚¬ìš©ëœ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ë³´ê¸°"):
            st.code(prompt)


def load_structured_context() -> Dict[str, str]:
    """Load structured rule/case context from the database to enrich chatbot prompts."""

    df = load_csv_from_db("parsed_docs")
    if df.empty:
        return {"rules": "", "cases": ""}

    df = df.fillna("")
    rule_sections: List[str] = []
    case_sections: List[str] = []

    rule_df = df[df["type"] != "case"]
    if not rule_df.empty:
        for category in RULE_CATEGORIES:
            category_df = rule_df[rule_df["category"] == category]
            if category_df.empty:
                continue
            lines = []
            for _, row in category_df.iterrows():
                summary = row.get("principle_summary") or row.get("content", "")
                priority = row.get("priority_tier", "")
                if priority:
                    lines.append(f"- ({priority}) {summary}")
                else:
                    lines.append(f"- {summary}")
            rule_sections.append(f"[{category}]\n" + "\n".join(lines))

    case_df = df[df["type"] == "case"]
    if not case_df.empty:
        for topic in CASE_FOCUS_TOPICS:
            topic_df = case_df[case_df["focus_topic"] == topic]
            if topic_df.empty:
                continue
            lines = []
            for _, row in topic_df.iterrows():
                chart = row.get("chart", "")
                principles = row.get("core_principles") or row.get("analysis") or row.get("content", "")
                lines.append(f"- {chart}: {principles}")
            case_sections.append(f"[{topic}]\n" + "\n".join(lines))

    return {"rules": "\n\n".join(rule_sections), "cases": "\n\n".join(case_sections)}


def build_interpretation_prompt(
    birth_year: str,
    birth_month: str,
    birth_day: str,
    birth_hour: str,
    gender: str,
    focus_topic: str,
    additional_question: str,
    context_payload: Dict[str, str],
) -> str:
    """Compose a structured prompt implementing the 3-step ìˆ˜ì•” ëª…ë¦¬ í•´ì„ í”„ë¡œì„¸ìŠ¤."""

    gender_text = gender if gender != "ì„ íƒ ì•ˆ í•¨" else "ë¯¸ì§€ì •"
    user_context = additional_question.strip() or "ì¶”ê°€ ì§ˆë¬¸ ì—†ìŒ"

    rule_context = context_payload.get("rules", "")
    case_context = context_payload.get("cases", "")

    prompt = f"""
ë‹¹ì‹ ì€ ìˆ˜ì•” ëª…ë¦¬ ìŠ¤íƒ€ì¼ì˜ ì „ë¬¸ í•´ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ë‹¨ê³„ì ì¸ ìƒë‹´ì„ ì œê³µí•©ë‹ˆë‹¤.

[ì‚¬ìš©ì ì…ë ¥]
- ìƒë…„ì›”ì¼ì‹œ: {birth_year}-{birth_month}-{birth_day} {birth_hour}
- ì„±ë³„: {gender_text}
- ì£¼ìš” ê´€ì‹¬ì‚¬: {focus_topic}
- ì¶”ê°€ ë°°ê²½/ì§ˆë¬¸: {user_context}

[ìˆ˜ì•” ëª…ë¦¬ í•µì‹¬ ì›ë¦¬ DB]
{rule_context or 'ë“±ë¡ëœ ì›ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê¸°ë³¸ ì›ë¦¬ì™€ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.'}

[ìˆ˜ì•” ëª…ë¦¬ ì‚¬ë¡€ DB]
{case_context or 'ë“±ë¡ëœ ì‚¬ë¡€ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¼ë°˜ì  ì‚¬ë¡€ ì¶”ë¡ ì„ ì ìš©í•˜ì„¸ìš”.'}

3ë‹¨ê³„ ì ˆì°¨ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.
1ë‹¨ê³„ (ëª…ì‹ ìƒì„± ë° ê¸°ë³¸ ë¶„ì„): ì…ë ¥ëœ ìƒë…„ì›”ì¼ì‹œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•œ ëª…ì‹ì„ ì¶”ì •í•˜ê³  ì‹­ì‹  ë°°ì¹˜ë¥¼ ê°œëµì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
2ë‹¨ê³„ (í•µì‹¬ ì¸ì ì¶”ì¶œ ë° ìš°ì„ ìˆœìœ„ ì ìš©): êµ¬ì¡°ë¡  ìš°ì„ ìˆœìœ„(1. ì œì••/í•©ì¶©í˜• ê³µ, 2. ê¶ìœ„Â·ëŒ€ìƒÂ·ë¬˜ê³  ë“± íŠ¹ìˆ˜ êµ¬ì¡°, 3. í—ˆíˆ¬/ì…ë¬˜, 4. ì¼ë°˜ ì‹­ì‹  í•´ì„)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ê²€í•˜ê³  í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
3ë‹¨ê³„ (ì‚¬ìš©ì ë§ì¶¤ í•´ì„): ì„ íƒëœ ê´€ì‹¬ ì£¼ì œ({focus_topic})ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì§ê´€ì ì¸ ìƒë‹´ ë¬¸ì¥ì„ ì œì‹œí•˜ê³ , í•„ìš”í•˜ë‹¤ë©´ ëŒ€ìš´/ì„¸ìš´ ë³€í™”ë¥¼ ê°„ë‹¨íˆ ì¡°ì–¸í•©ë‹ˆë‹¤.

ê° ë‹¨ê³„ë¥¼ ëª…í™•í•œ ì†Œì œëª©ìœ¼ë¡œ êµ¬ë¶„í•˜ê³ , ìˆ˜ì•” ëª…ë¦¬ ìš©ì–´ë¥¼ í•œêµ­ì–´ë¡œ ìœ ì§€í•˜ë©´ì„œ ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë§ˆì§€ë§‰ì—ëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë¶ˆë¦¿ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
"""

    return prompt.strip()


def render_ai_consultation_section() -> None:
    st.header("ğŸ’¬ AI ìƒë‹´")

    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="user_query")
    search_mode = st.radio(
        "ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ", ["ë²¡í„° ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"], horizontal=True
    )

    if not st.button("AI ì‘ë‹µ ìƒì„±"):
        return

    if not query.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    if search_mode == "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰":
        docs = hybrid_search(query, db_dir=VECTOR_DB_DIR, k=5)
        context = "\n\n".join([doc.page_content for doc in docs])
        answer = generate_ai_response(f"{query}\n\nì°¸ê³ ìë£Œ:\n{context}")
    else:
        answer = generate_ai_response(query)

    st.markdown(answer)


def render_database_build_section() -> None:
    st.header("ğŸ› ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ")

    if not st.button("ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ ì‹¤í–‰"):
        return

    with st.spinner("ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê³  DB/VectorDBë¥¼ ë¹Œë“œ ì¤‘..."):
        vs = build_databases(data_dir=RAW_DOCS_DIR, db_dir=VECTOR_DB_DIR)

    if vs:
        st.success("âœ… DB ë° VectorDB ë¹Œë“œ ì™„ë£Œ")
    else:
        st.warning("âš ï¸ ë¹Œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")


def render_database_management_section() -> None:
    st.header("ğŸ—‚ï¸ DB ê´€ë¦¬")

    if st.button("í…Œì´ë¸” ëª©ë¡ ë³´ê¸°"):
        tables = list_tables()
        if tables:
            st.write("ğŸ“‹ í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:")
            st.write(tables)
        else:
            st.info("DBì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")

    tables = list_tables()
    if not tables:
        return

    selected_table = st.selectbox("ì¡°íšŒí•  í…Œì´ë¸” ì„ íƒ", tables, key="view_table")
    if st.button("í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°"):
        df = load_csv_from_db(selected_table)
        if df.empty:
            st.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.dataframe(df, use_container_width=True)

    del_table = st.selectbox("ì‚­ì œí•  í…Œì´ë¸” ì„ íƒ", tables, key="delete_table")
    if st.button("í…Œì´ë¸” ì‚­ì œ"):
        drop_table(del_table)


def drop_table(table_name: str) -> None:
    """Safely drop a table from the SQLite database used by the app."""

    import sqlite3

    with sqlite3.connect("suri_m.db") as conn:
        cursor = conn.cursor()
        cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
        conn.commit()

    st.success(f"ğŸ—‘ï¸ {table_name} í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")


def main() -> None:
    configure_app()
    page_choice = st.sidebar.radio("ğŸ“Œ í˜ì´ì§€ ì„ íƒ", ["ë¬¸ì„œ ê´€ë¦¬", "ì¸ë¬¼ í”„ë¡œí•„"])

    if page_choice == "ë¬¸ì„œ ê´€ë¦¬":
        render_document_management_page()
    else:
        profiles_page()


if __name__ == "__main__":
    main()
