import streamlit as st
import os
import pandas as pd
from io import StringIO

# =============================
# core ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
# =============================
from core.database import (
    ensure_db,
    insert_csv_to_db,
    load_csv_from_db,
    load_csv_files,
    list_tables,
)
from core.hybrid_search import hybrid_search
from core.ai_engine import (
    generate_ai_response,
    ask_csv_ai,
    summarize_long_csv,
    summarize_by_keywords,
    clean_text_with_ai,
)
from core.parsing import parse_and_store_documents
from core.rag import build_databases

# =============================
# ì´ˆê¸° ì„¸íŒ…
# =============================
st.set_page_config(page_title="Suri Q&AI", layout="wide")
st.title("ğŸ“Š Suri Q&AI (ìµœì‹  OpenAI API ë²„ì „)")

# ğŸ”¹ í´ë” ì´ˆê¸°í™”
for path in ["data", "data/raw_docs", "data/vector_db"]:
    os.makedirs(path, exist_ok=True)

# ğŸ”¹ DB ì´ˆê¸°í™”
ensure_db()

# =============================
# 1. ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±
# =============================
st.header("ğŸ“‘ ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±")

uploaded_files = st.file_uploader(
    "txt/md íŒŒì¼ ì—…ë¡œë“œ", type=["txt", "md"], accept_multiple_files=True
)

if uploaded_files:
    for uploaded_file in uploaded_files:
        file_content = uploaded_file.read().decode("utf-8")
        st.subheader(f"ğŸ“„ {uploaded_file.name}")
        st.text_area("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", file_content[:1000], height=200)

        if st.button(f"ì´ ë¬¸ì„œ íŒŒì‹±í•˜ê¸°: {uploaded_file.name}"):
            save_path = os.path.join("data/raw_docs", uploaded_file.name)
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(file_content)

            parsed_df = parse_and_store_documents(save_path)

            if parsed_df is not None and isinstance(parsed_df, pd.DataFrame) and not parsed_df.empty:
                st.success("âœ… íŒŒì‹± ì™„ë£Œ, AI êµì • ì ìš© ì¤‘...")

                raw_text = parsed_df.to_csv(index=False, encoding="utf-8-sig")
                cleaned_text = clean_text_with_ai(raw_text)

                try:
                    cleaned_df = pd.read_csv(StringIO(cleaned_text))
                except Exception as e:
                    st.error(f"AI êµì • í›„ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")
                    cleaned_df = parsed_df

                st.success("âœ… AI êµì • ì™„ë£Œ! ì•„ë˜ì—ì„œ ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•˜ì„¸ìš”.")
                edited_df = st.data_editor(cleaned_df, num_rows="dynamic", use_container_width=True)

                if st.button(f"{uploaded_file.name} ì €ì¥", key=f"save_{uploaded_file.name}"):
                    parsed_csv = "data/parsed_docs.csv"
                    if os.path.exists(parsed_csv):
                        old_df = pd.read_csv(parsed_csv)
                        combined = pd.concat([old_df, edited_df], ignore_index=True).drop_duplicates()
                    else:
                        combined = edited_df

                    combined.to_csv(parsed_csv, index=False, encoding="utf-8-sig")
                    st.success("ğŸ“‚ parsed_docs.csv ì €ì¥ ì™„ë£Œ âœ…")

                    insert_csv_to_db(combined, table_name="parsed_docs")
                    st.success("ğŸ“¦ DBì—ë„ ì €ì¥ ì™„ë£Œ âœ…")
            else:
                st.warning("âš ï¸ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# =============================
# 2. CSV ë°ì´í„° ê´€ë¦¬
# =============================
st.header("ğŸ“‚ CSV ë°ì´í„° ê´€ë¦¬")
csv_dfs = load_csv_files("data")

if st.button("DBì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"):
    db_df = load_csv_from_db("parsed_docs")
    if db_df is not None and not db_df.empty:
        st.subheader("ğŸ“¦ DB ë¶ˆëŸ¬ì˜¤ê¸° ê²°ê³¼")
        st.dataframe(db_df, use_container_width=True)

if not csv_dfs:
    st.info("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì—…ë¡œë“œ/íŒŒì‹±ì„ ì§„í–‰í•˜ì„¸ìš”.")
else:
    for name, df in csv_dfs.items():
        st.subheader(f"ğŸ“‘ {name}.csv")

        csv_text = df.to_csv(index=False, encoding="utf-8-sig")

        if st.button(f"{name}.csv AI êµì • ì ìš©", key=f"clean_{name}"):
            st.info("AI êµì • ì¤‘...")
            cleaned_text = clean_text_with_ai(csv_text)
            try:
                df = pd.read_csv(StringIO(cleaned_text))
                st.success("âœ… AI êµì • ì™„ë£Œ! ì•„ë˜ì—ì„œ ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•˜ì„¸ìš”.")
            except Exception as e:
                st.error(f"AI êµì • í›„ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")

        edited_df = st.data_editor(df, num_rows="dynamic", use_container_width=True)

        if st.button(f"{name}.csv ì €ì¥", key=f"save_{name}"):
            save_path = f"data/{name}.csv"
            edited_df.to_csv(save_path, index=False, encoding="utf-8-sig")
            st.success(f"{name}.csv ì €ì¥ ì™„ë£Œ âœ…")

            insert_csv_to_db(edited_df, table_name=name)
            st.success(f"ğŸ“¦ {name} â†’ DB ì €ì¥ ì™„ë£Œ âœ…")

# =============================
# 3. AI ìƒë‹´ (ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
# =============================
st.header("ğŸ’¬ AI ìƒë‹´")

query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="user_query")
search_mode = st.radio("ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ", ["ë²¡í„° ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"], horizontal=True)

if st.button("AI ì‘ë‹µ ìƒì„±"):
    if query.strip():
        if search_mode == "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰":
            docs = hybrid_search(query, db_dir="data/vector_db", k=5)
            context = "\n\n".join([d.page_content for d in docs])
            answer = generate_ai_response(f"{query}\n\nì°¸ê³ ìë£Œ:\n{context}")
        else:
            answer = generate_ai_response(query)
        st.markdown(answer)
    else:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# =============================
# 4. CSV ìš”ì•½
# =============================
st.header("ğŸ“ CSV ìš”ì•½")
if st.button("CSV ì „ì²´ ìš”ì•½"):
    if not csv_dfs:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        try:
            combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
            csv_text = combined_df.to_csv(index=False, encoding="utf-8-sig")
            summary, parts = summarize_long_csv(csv_text)
            st.text_area("CSV ì „ì²´ ìš”ì•½ ê²°ê³¼", summary, height=300)
            with st.expander("ë¶€ë¶„ ìš”ì•½ ë³´ê¸°"):
                for part in parts:
                    st.markdown(part)
        except ValueError as exc:
            st.error(f"CSV ê²°í•© ì˜¤ë¥˜: {exc}")

# =============================
# 5. í‚¤ì›Œë“œë³„ ì •ë¦¬
# =============================
st.header("ğŸ”‘ í‚¤ì›Œë“œë³„ ë¬¸ì„œ ì •ë¦¬")

keywords_input = st.text_input("í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¬ë¬¼, í˜¼ì¸, ì§ì¥, ê±´ê°•)")
if st.button("í‚¤ì›Œë“œë³„ ì •ë¦¬ ì‹¤í–‰"):
    if not csv_dfs:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
        csv_text = combined_df.to_csv(index=False, encoding="utf-8-sig")
        keywords = [kw.strip() for kw in keywords_input.split(",") if kw.strip()]
        if not keywords:
            st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            summary_by_kw = summarize_by_keywords(csv_text, keywords)
            st.text_area("í‚¤ì›Œë“œë³„ ì •ë¦¬ ê²°ê³¼", summary_by_kw, height=400)

# =============================
# 6. DB ë¹Œë“œ
# =============================
st.header("ğŸ› ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ")

if st.button("ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ ì‹¤í–‰"):
    with st.spinner("ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê³  DB/VectorDBë¥¼ ë¹Œë“œ ì¤‘..."):
        vs = build_databases(data_dir="data/raw_docs", db_dir="data/vector_db")
    if vs:
        st.success("âœ… DB ë° VectorDB ë¹Œë“œ ì™„ë£Œ")
    else:
        st.warning("âš ï¸ ë¹Œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data/raw_docsì— íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

# =============================
# 7. Parsed CSV ìˆœì„œ ì¡°ì •
# =============================
st.header("ğŸ“‚ Parsed CSV ìˆœì„œ ì¡°ì •")

parsed_csv = "data/parsed_docs.csv"
if os.path.exists(parsed_csv):
    df = pd.read_csv(parsed_csv)

    if "order" not in df.columns:
        df.insert(0, "order", range(1, len(df) + 1))

    st.info("â„¹ï¸ order ì»¬ëŸ¼ì„ ìˆ˜ì •í•´ì„œ ìˆœì„œë¥¼ ë°”ê¿”ì£¼ì„¸ìš”.")
    edited_df = st.data_editor(df, num_rows="dynamic", use_container_width=True)

    if st.button("ìˆœì„œ ì ìš© & ì €ì¥"):
        sorted_df = edited_df.sort_values("order").reset_index(drop=True)
        sorted_df.to_csv(parsed_csv, index=False, encoding="utf-8-sig")
        st.success("âœ… ìˆœì„œ ë°˜ì˜ í›„ ì €ì¥ ì™„ë£Œ")

        insert_csv_to_db(sorted_df, table_name="parsed_docs")
        st.success("ğŸ“¦ DBì—ë„ ì €ì¥ ì™„ë£Œ âœ…")
else:
    st.info("parsed_docs.csv íŒŒì¼ì´ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œ/íŒŒì‹±í•˜ì„¸ìš”.")

# =============================
# 8. DB ê´€ë¦¬ (list_tables í™œìš©)
# =============================
st.header("ğŸ—‚ï¸ DB ê´€ë¦¬")

if st.button("í…Œì´ë¸” ëª©ë¡ ë³´ê¸°"):
    tables = list_tables()
    if tables:
        st.write("ğŸ“‹ í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:")
        st.write(tables)
    else:
        st.info("DBì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")

tables = list_tables()
if tables:
    selected_table = st.selectbox("ì¡°íšŒí•  í…Œì´ë¸” ì„ íƒ", tables, key="view_table")
    if st.button("í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°"):
        df = load_csv_from_db(selected_table)
        if not df.empty:
            st.dataframe(df, use_container_width=True)
        else:
            st.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    del_table = st.selectbox("ì‚­ì œí•  í…Œì´ë¸” ì„ íƒ", tables, key="delete_table")
    if st.button("í…Œì´ë¸” ì‚­ì œ"):
        import sqlite3
        conn = sqlite3.connect("suri_m.db")  # DB_PATHì™€ ë™ì¼í•´ì•¼ í•¨
        cur = conn.cursor()
        cur.execute(f"DROP TABLE IF EXISTS {del_table}")
        conn.commit()
        conn.close()
        st.success(f"ğŸ—‘ï¸ {del_table} í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")
