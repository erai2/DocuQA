import streamlit as st
import os
import pandas as pd
from io import StringIO

# Core ëª¨ë“ˆ
from core.database import ensure_db, insert_csv_to_db, load_csv_from_db, load_csv_files
from core.hybrid_search import hybrid_search
from core.ai_engine import (
    generate_ai_response,
    ask_csv_ai,
    summarize_long_csv,
    summarize_by_keywords,
    clean_text_with_ai,
)
from core.parsing import parse_and_store_documents

# =============================
# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
# =============================
st.set_page_config(page_title="Suri Q&AI", layout="wide")
st.title("ğŸ“Š Suri Q&AI (ìµœì‹  OpenAI API ë²„ì „)")

# =============================
# 1. ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±
# =============================
st.header("ğŸ“‘ ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±")

uploaded_files = st.file_uploader(
    "txt/md íŒŒì¼ ì—…ë¡œë“œ", type=["txt", "md"], accept_multiple_files=True
)

if uploaded_files:
    for uploaded_file in uploaded_files:
        file_content = uploaded_file.read().decode("utf-8")
        st.subheader(f"ğŸ“„ {uploaded_file.name}")
        st.text_area("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", file_content[:1000], height=200)

        if st.button(f"ì´ ë¬¸ì„œ íŒŒì‹±í•˜ê¸°: {uploaded_file.name}"):
            save_path = os.path.join("data/raw_docs", uploaded_file.name)
            os.makedirs("data/raw_docs", exist_ok=True)
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(file_content)

            parsed_df = parse_and_store_documents(save_path)

            if parsed_df is not None and isinstance(parsed_df, pd.DataFrame) and not parsed_df.empty:
                st.success("âœ… íŒŒì‹± ì™„ë£Œ, AI êµì • ì ìš© ì¤‘...")

                # DataFrame â†’ CSV ë¬¸ìì—´ ë³€í™˜
                raw_text = parsed_df.to_csv(index=False, encoding="utf-8-sig")

                # AI êµì •
                cleaned_text = clean_text_with_ai(raw_text)

                try:
                    cleaned_df = pd.read_csv(StringIO(cleaned_text))
                except Exception as e:
                    st.error(f"AI êµì • í›„ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")
                    cleaned_df = parsed_df

                st.success("âœ… AI êµì • ì™„ë£Œ! ì•„ë˜ì—ì„œ ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•˜ì„¸ìš”.")
                edited_df = st.data_editor(cleaned_df, num_rows="dynamic", width="stretch")

                if st.button(f"{uploaded_file.name} ì €ì¥", key=f"save_{uploaded_file.name}"):
                    parsed_csv = "data/parsed_docs.csv"
                    if os.path.exists(parsed_csv):
                        old_df = pd.read_csv(parsed_csv)
                        combined = pd.concat([old_df, edited_df], ignore_index=True).drop_duplicates()
                    else:
                        combined = edited_df

                    combined.to_csv(parsed_csv, index=False, encoding="utf-8-sig")
                    st.success("ğŸ“‚ parsed_docs.csv ì €ì¥ ì™„ë£Œ âœ…")

                    # ğŸ”¹ DBì—ë„ ë°˜ì˜
                    ensure_db()
                    insert_csv_to_db(combined, table_name="parsed_docs")
                    st.success("ğŸ“¦ DBì—ë„ ì €ì¥ ì™„ë£Œ âœ…")
            else:
                st.warning("âš ï¸ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# =============================
# 2. CSV ë°ì´í„° ê´€ë¦¬
# =============================
st.header("ğŸ“‚ CSV ë°ì´í„° ê´€ë¦¬")
csv_dfs = load_csv_files("data")

# ğŸ”¹ DB ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥
if st.button("DBì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"):
    ensure_db()
    db_df = load_csv_from_db("parsed_docs")
    if db_df is not None and not db_df.empty:
        st.subheader("ğŸ“¦ DB ë¶ˆëŸ¬ì˜¤ê¸° ê²°ê³¼")
        st.dataframe(db_df, width="stretch")

if not csv_dfs:
    st.info("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì—…ë¡œë“œ/íŒŒì‹±ì„ ì§„í–‰í•˜ì„¸ìš”.")
else:
    for name, df in csv_dfs.items():
        st.subheader(f"ğŸ“‘ {name}.csv")

        # ì›ë³¸ CSV â†’ ë¬¸ìì—´ ë³€í™˜
        csv_text = df.to_csv(index=False, encoding="utf-8-sig")

        if st.button(f"{name}.csv AI êµì • ì ìš©", key=f"clean_{name}"):
            st.info("AI êµì • ì¤‘...")
            cleaned_text = clean_text_with_ai(csv_text)
            try:
                df = pd.read_csv(StringIO(cleaned_text))
                st.success("âœ… AI êµì • ì™„ë£Œ! ì•„ë˜ì—ì„œ ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•˜ì„¸ìš”.")
            except Exception as e:
                st.error(f"AI êµì • í›„ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")

        edited_df = st.data_editor(df, num_rows="dynamic", width="stretch")

        if st.button(f"{name}.csv ì €ì¥", key=f"save_{name}"):
            save_path = f"data/{name}.csv"
            edited_df.to_csv(save_path, index=False, encoding="utf-8-sig")
            st.success(f"{name}.csv ì €ì¥ ì™„ë£Œ âœ…")

            # DBì—ë„ ì €ì¥
            ensure_db()
            insert_csv_to_db(edited_df, table_name=name)
            st.success(f"ğŸ“¦ {name} â†’ DB ì €ì¥ ì™„ë£Œ âœ…")

# =============================
# 3. AI ìƒë‹´ (ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
# =============================
st.header("ğŸ’¬ AI ìƒë‹´")

query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="user_query")
search_mode = st.radio("ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ", ["ë²¡í„° ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"], horizontal=True)

if st.button("AI ì‘ë‹µ ìƒì„±"):
    if query.strip():
        if search_mode == "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰":
            docs = hybrid_search(query, db_dir="data/vector_db", k=5)
            context = "\n\n".join([d.page_content for d in docs])
            answer = generate_ai_response(f"{query}\n\nì°¸ê³ ìë£Œ:\n{context}")
        else:
            answer = generate_ai_response(query)
        st.markdown(answer)
    else:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# =============================
# 4. CSV ìš”ì•½
# =============================
st.header("ğŸ“ CSV ìš”ì•½")
if st.button("CSV ì „ì²´ ìš”ì•½"):
    if not csv_dfs:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        try:
            combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
            csv_text = combined_df.to_csv(index=False, encoding="utf-8-sig")
            summary, parts = summarize_long_csv(csv_text)
            st.text_area("CSV ì „ì²´ ìš”ì•½ ê²°ê³¼", summary, height=300)
            with st.expander("ë¶€ë¶„ ìš”ì•½ ë³´ê¸°"):
                for part in parts:
                    st.markdown(part)
        except ValueError as exc:
            st.error(f"CSV ê²°í•© ì˜¤ë¥˜: {exc}")

# =============================
# 5. í‚¤ì›Œë“œë³„ ì •ë¦¬
# =============================
st.header("ğŸ”‘ í‚¤ì›Œë“œë³„ ë¬¸ì„œ ì •ë¦¬")

keywords_input = st.text_input("í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¬ë¬¼, í˜¼ì¸, ì§ì¥, ê±´ê°•)")
if st.button("í‚¤ì›Œë“œë³„ ì •ë¦¬ ì‹¤í–‰"):
    if not csv_dfs:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
        csv_text = combined_df.to_csv(index=False, encoding="utf-8-sig")
        keywords = [kw.strip() for kw in keywords_input.split(",") if kw.strip()]
        if not keywords:
            st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            summary_by_kw = summarize_by_keywords(csv_text, keywords)
            st.text_area("í‚¤ì›Œë“œë³„ ì •ë¦¬ ê²°ê³¼", summary_by_kw, height=400)
