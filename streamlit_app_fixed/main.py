"""Streamlit entrypoint for the Suri Q&AI application."""

from __future__ import annotations

import os
from io import StringIO
from typing import Dict, Iterable, List

import pandas as pd
import streamlit as st

from core.ai_engine import generate_ai_response, summarize_by_keywords, summarize_long_csv
from core.ai_utils import clean_text_with_ai
from core.database import (
    ensure_db,
    insert_csv_to_db,
    load_csv_files,
    load_csv_from_db,
    list_tables,
)
from core.rag import build_databases
from profiles_page import profiles_page


DATA_DIR = "data"
RAW_DOCS_DIR = os.path.join(DATA_DIR, "raw_docs")
VECTOR_DB_DIR = os.path.join(DATA_DIR, "vector_db")
PARSED_CSV_PATH = os.path.join(DATA_DIR, "parsed_docs.csv")


def configure_app() -> None:
    """Set default layout information and bootstrap directories/database."""

    st.set_page_config(page_title="Suri Q&AI", layout="wide")
    st.title("ğŸ“Š Suri Q&AI (ìµœì‹  OpenAI API ë²„ì „)")

    ensure_directories([DATA_DIR, RAW_DOCS_DIR, VECTOR_DB_DIR])
    ensure_db()


def ensure_directories(paths: Iterable[str]) -> None:
    """Create directories required by the application if they do not exist."""

    for path in paths:
        os.makedirs(path, exist_ok=True)


def render_document_management_page() -> None:
    """Render the document management workflow."""

    render_upload_section()
    render_db_preview_section()
    render_csv_summary_section()
    render_keyword_summary_section()
    render_ai_consultation_section()
    render_database_build_section()
    render_database_management_section()


def render_upload_section() -> None:
    st.header("ğŸ“‘ ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±")

    parser_mode = st.radio(
        "íŒŒì„œ ëª¨ë“œ ì„ íƒ",
        ["1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ (ë¹ ë¦„)", "2ë‹¨ê³„: AI ë³´ì¡° (ì •ë°€)", "3ë‹¨ê³„: Hybrid (íš¨ìœ¨ì )"],
        horizontal=True,
    )

    uploaded_files = st.file_uploader(
        "txt/md íŒŒì¼ ì—…ë¡œë“œ", type=["txt", "md"], accept_multiple_files=True
    )

    if not uploaded_files:
        return

    for uploaded_file in uploaded_files:
        file_content = uploaded_file.read().decode("utf-8")
        st.subheader(f"ğŸ“„ {uploaded_file.name}")
        st.text_area("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", file_content[:1000], height=200)

        if not st.button(f"ì´ ë¬¸ì„œ íŒŒì‹±í•˜ê¸°: {uploaded_file.name}"):
            continue

        save_path = os.path.join(RAW_DOCS_DIR, uploaded_file.name)
        with open(save_path, "w", encoding="utf-8") as file:
            file.write(file_content)

        parsed_df = parse_document_by_mode(file_content, parser_mode)
        if parsed_df.empty:
            st.warning("âš ï¸ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        st.success("âœ… íŒŒì‹± ì™„ë£Œ, AI êµì • ì ìš© ì¤‘...")

        raw_text = parsed_df.to_csv(index=False, encoding="utf-8-sig")
        cleaned_df = clean_dataframe_with_ai(raw_text, parsed_df)

        st.success("âœ… AI êµì • ì™„ë£Œ! ì•„ë˜ì—ì„œ ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•˜ì„¸ìš”.")
        edited_df = st.data_editor(cleaned_df, num_rows="dynamic", use_container_width=True)

        if st.button(f"{uploaded_file.name} ì €ì¥", key=f"save_{uploaded_file.name}"):
            combined = merge_with_existing_csv(edited_df)
            combined.to_csv(PARSED_CSV_PATH, index=False, encoding="utf-8-sig")
            st.success(f"ğŸ“‚ parsed_docs.csv ì €ì¥ ì™„ë£Œ (ì´ {len(combined)}í–‰) âœ…")

            total_rows = insert_csv_to_db(combined, table_name="parsed_docs")
            st.success(f"ğŸ“¦ DB ì €ì¥ ì™„ë£Œ: {total_rows}í–‰ (ì¤‘ë³µ ì œê±° í›„)")


def parse_document_by_mode(file_content: str, parser_mode: str) -> pd.DataFrame:
    """Parse documents using the selected parsing strategy."""

    rows: List[Dict[str, str]] = []

    if "ê·œì¹™ ê¸°ë°˜" in parser_mode:
        from core.parsing import parse_document

        cases, rules, concepts = parse_document(file_content)
    elif "AI ë³´ì¡°" in parser_mode:
        from core.parse_document_ml import parse_document_ml

        cases, rules, concepts = parse_document_ml(file_content)
    else:
        from core.parse_document_hybrid import parse_document_hybrid

        cases, rules, concepts = parse_document_hybrid(file_content)

    for case in cases:
        rows.append({"type": "case", "id": case["id"], "content": case.get("detail", "")})
    for rule in rules:
        rows.append({"type": "rule", "id": rule["id"], "content": rule.get("desc", "")})
    for concept in concepts:
        rows.append({"type": "concept", "id": concept["id"], "content": concept.get("desc", "")})

    return pd.DataFrame(rows)


def clean_dataframe_with_ai(raw_text: str, fallback_df: pd.DataFrame) -> pd.DataFrame:
    """Apply AI-powered cleaning to the parsed dataframe, falling back to original data."""

    cleaned_text = clean_text_with_ai(raw_text)
    try:
        return pd.read_csv(StringIO(cleaned_text))
    except Exception as exc:  # pragma: no cover - defensive logging for UI
        st.error(f"AI êµì • í›„ CSV ë³€í™˜ ì‹¤íŒ¨: {exc}")
        return fallback_df


def merge_with_existing_csv(edited_df: pd.DataFrame) -> pd.DataFrame:
    """Merge edited dataframe with existing parsed CSV data, removing duplicates."""

    if os.path.exists(PARSED_CSV_PATH):
        old_df = pd.read_csv(PARSED_CSV_PATH)
        combined = pd.concat([old_df, edited_df], ignore_index=True)
        combined = combined.drop_duplicates(subset=["type", "id", "content"])
        return combined

    return edited_df


def render_db_preview_section() -> None:
    st.header("ğŸ“¦ DB ë°ì´í„° í™•ì¸")

    if not st.button("DBì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"):
        return

    db_df = load_csv_from_db("parsed_docs")
    if db_df.empty:
        st.warning("âš ï¸ DBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.subheader("ğŸ“¦ DB ë¶ˆëŸ¬ì˜¤ê¸° ê²°ê³¼")
    st.dataframe(db_df, use_container_width=True)


def render_csv_summary_section() -> None:
    st.header("ğŸ“ CSV ìš”ì•½")

    if not st.button("CSV ì „ì²´ ìš”ì•½"):
        return

    csv_text = build_combined_csv_text()
    if not csv_text:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        summary, parts = summarize_long_csv(csv_text)
    except ValueError as exc:
        st.error(f"CSV ê²°í•© ì˜¤ë¥˜: {exc}")
        return

    st.text_area("CSV ì „ì²´ ìš”ì•½ ê²°ê³¼", summary, height=300)
    with st.expander("ë¶€ë¶„ ìš”ì•½ ë³´ê¸°"):
        for part in parts:
            st.markdown(part)


def build_combined_csv_text() -> str:
    """Combine CSV files from the data directory into a single CSV text blob."""

    csv_dfs = load_csv_files(DATA_DIR)
    if not csv_dfs:
        return ""

    combined_df = pd.concat(list(csv_dfs.values()), ignore_index=True)
    return combined_df.to_csv(index=False, encoding="utf-8-sig")


def render_keyword_summary_section() -> None:
    st.header("ğŸ”‘ í‚¤ì›Œë“œë³„ ë¬¸ì„œ ì •ë¦¬")
    keywords_input = st.text_input("í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆ(,)ë¡œ ì…ë ¥ (ì˜ˆ: ì¬ë¬¼, í˜¼ì¸, ì§ì¥, ê±´ê°•)")

    if not st.button("í‚¤ì›Œë“œë³„ ì •ë¦¬ ì‹¤í–‰"):
        return

    csv_text = build_combined_csv_text()
    if not csv_text:
        st.warning("CSV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    keywords = [keyword.strip() for keyword in keywords_input.split(",") if keyword.strip()]
    if not keywords:
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return

    summary_by_kw = summarize_by_keywords(csv_text, keywords)
    st.text_area("í‚¤ì›Œë“œë³„ ì •ë¦¬ ê²°ê³¼", summary_by_kw, height=400)


def render_ai_consultation_section() -> None:
    st.header("ğŸ’¬ AI ìƒë‹´")

    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="user_query")
    search_mode = st.radio(
        "ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ", ["ë²¡í„° ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"], horizontal=True
    )

    if not st.button("AI ì‘ë‹µ ìƒì„±"):
        return

    if not query.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    if search_mode == "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰":
        docs = hybrid_search(query, db_dir=VECTOR_DB_DIR, k=5)
        context = "\n\n".join([doc.page_content for doc in docs])
        answer = generate_ai_response(f"{query}\n\nì°¸ê³ ìë£Œ:\n{context}")
    else:
        answer = generate_ai_response(query)

    st.markdown(answer)


def render_database_build_section() -> None:
    st.header("ğŸ› ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ")

    if not st.button("ë°ì´í„°ë² ì´ìŠ¤ ë¹Œë“œ ì‹¤í–‰"):
        return

    with st.spinner("ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê³  DB/VectorDBë¥¼ ë¹Œë“œ ì¤‘..."):
        vs = build_databases(data_dir=RAW_DOCS_DIR, db_dir=VECTOR_DB_DIR)

    if vs:
        st.success("âœ… DB ë° VectorDB ë¹Œë“œ ì™„ë£Œ")
    else:
        st.warning("âš ï¸ ë¹Œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")


def render_database_management_section() -> None:
    st.header("ğŸ—‚ï¸ DB ê´€ë¦¬")

    if st.button("í…Œì´ë¸” ëª©ë¡ ë³´ê¸°"):
        tables = list_tables()
        if tables:
            st.write("ğŸ“‹ í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:")
            st.write(tables)
        else:
            st.info("DBì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")

    tables = list_tables()
    if not tables:
        return

    selected_table = st.selectbox("ì¡°íšŒí•  í…Œì´ë¸” ì„ íƒ", tables, key="view_table")
    if st.button("í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°"):
        df = load_csv_from_db(selected_table)
        if df.empty:
            st.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.dataframe(df, use_container_width=True)

    del_table = st.selectbox("ì‚­ì œí•  í…Œì´ë¸” ì„ íƒ", tables, key="delete_table")
    if st.button("í…Œì´ë¸” ì‚­ì œ"):
        drop_table(del_table)


def drop_table(table_name: str) -> None:
    """Safely drop a table from the SQLite database used by the app."""

    import sqlite3

    with sqlite3.connect("suri_m.db") as conn:
        cursor = conn.cursor()
        cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
        conn.commit()

    st.success(f"ğŸ—‘ï¸ {table_name} í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")


def main() -> None:
    configure_app()
    page_choice = st.sidebar.radio("ğŸ“Œ í˜ì´ì§€ ì„ íƒ", ["ë¬¸ì„œ ê´€ë¦¬", "ì¸ë¬¼ í”„ë¡œí•„"])

    if page_choice == "ë¬¸ì„œ ê´€ë¦¬":
        render_document_management_page()
    else:
        profiles_page()


if __name__ == "__main__":
    main()
